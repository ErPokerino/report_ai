---
title: "Report Analisi Lucy - Sistema Riconoscimento Documentale"
author: "Sistema Automatico"
params:
  dataset_path: "../data/lucy_data.csv"
  enable_ai: true
---

## Introduzione

```{python}
#| echo: false
#| output: asis

from IPython.display import Markdown, display

# Costruisci il testo dell'introduzione
intro_text = "Questo report analizza i dati del sistema Lucy, applicativo documentale per la gestione delle fatture di Luxottica. Il sistema utilizza algoritmi di riconoscimento automatico (incluso ML con XGBoost) per estrarre informazioni dalle fatture, estraendo **tutti i campi** indicati in `field_name`.\n\n"
intro_text += "Le analisi e i commenti sono generati automaticamente utilizzando **LangChain** e modelli LLM. Il sistema utilizza la seguente gerarchia di modelli con fallback automatico: **GPT-5.2 → Gemini 3 PRO → Gemini Flash 2.5 → GPT-4o**. Il modello effettivamente utilizzato per generare questo report viene indicato nella sezione 'Modelli LLM Utilizzati' alla fine del documento."

display(Markdown(intro_text))
```

## Caricamento Dati

```{python}
#| echo: false
#| code-fold: true
#| output: asis

import sys
sys.path.append('../src')

from data_loader import load_csv_data, calculate_metrics_by_method, get_field_names, filter_by_field_name, calculate_percentage
from table_formatter import format_table, format_summary_dict
import pandas as pd
from IPython.display import Markdown, display

# Carica i dati Lucy
try:
    dataset_path = params.dataset_path if hasattr(params, 'dataset_path') and params.dataset_path else "../data/lucy_data.csv"
except (NameError, AttributeError):
    dataset_path = "../data/lucy_data.csv"

df = load_csv_data(dataset_path)

# Statistiche base
summary_data = {
    'Righe': f"{df.shape[0]:,}",
    'Colonne': f"{df.shape[1]}",
    'Periodo': f"{df['datetime_sent'].min()} - {df['datetime_sent'].max()}",
    'Record validati': f"{df['is_validated'].sum():,} ({df['is_validated'].sum()/len(df)*100:.1f}%)",
    'Metodi utilizzati': f"{df['method_pred'].nunique()}",
    'Protocolli unici': f"{df['protocol'].nunique():,}",
    'Fornitori unici': f"{df['id_company'].nunique()}"
}
result = format_summary_dict(summary_data, "Statistiche Dati", return_string=True)
print(result)

# Distribuzione field_name
if 'field_name' in df.columns:
    field_names = get_field_names(df)
    field_dist = df['field_name'].value_counts()
    
    # Statistiche per ogni field_name
    field_stats = []
    for fn in field_names:
        field_df = filter_by_field_name(df, fn)
        field_stats.append({
            'field_name': fn,
            'Totale record': len(field_df),
            'Validati': field_df['is_validated'].sum(),
            '% Validati': f"{calculate_percentage(field_df['is_validated'].sum(), len(field_df), decimal_places=1):.1f}%" if len(field_df) > 0 else "0%",
            'Metodi utilizzati': field_df['method_pred'].nunique() if 'method_pred' in field_df.columns else 0
        })
    
    field_stats_df = pd.DataFrame(field_stats)
    result = format_table(field_stats_df, "Distribuzione e Statistiche per Campo (field_name)", return_string=True)
    print(result)

# Prime righe
result = format_table(df.head(), "Prime righe del dataset", return_string=True)
print(result)
```

### Distribuzione Campi (field_name)

```{python}
#| echo: false
#| label: fig-field-distribution
#| fig-cap: "Distribuzione dei campi (field_name) nel dataset"
#| fig-width: 10
#| fig-height: 6
#| fig-pos: "H"

from lucy_visualizations import plot_field_name_distribution

if 'field_name' in df.columns:
    fig = plot_field_name_distribution(df)
else:
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.text(0.5, 0.5, 'Colonna field_name non presente', ha='center', va='center')
```

### Riassunto Analitico Generale

```{python}
#| echo: false
#| code-fold: true
#| warning: false

import os
from ai_analysis import analyze_data_summary
from IPython.display import Markdown, display

ai_summary = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        ai_summary = analyze_data_summary(df)
    elif not enable_ai:
        ai_summary = "Analisi AI disabilitata tramite parametro."
    else:
        ai_summary = "OPENAI_API_KEY non configurata. Configura la variabile d'ambiente per abilitare l'analisi AI."
except (NameError, AttributeError) as e:
    # params non disponibile, usa default
    if os.getenv("OPENAI_API_KEY"):
        ai_summary = analyze_data_summary(df)
    else:
        ai_summary = "OPENAI_API_KEY non configurata. Configura la variabile d'ambiente per abilitare l'analisi AI."
except Exception as e:
    ai_summary = f"Errore nell'analisi AI: {str(e)}"
```

```{python}
#| echo: false
#| code-fold: true
#| output: asis

if ai_summary:
    print(ai_summary)
```

## Overview Dati

### Distribuzione Metodi

```{python}
#| echo: false
#| label: fig-method-usage
#| fig-cap: "Distribuzione dell'uso dei metodi di riconoscimento"
#| fig-width: 10
#| fig-height: 6
#| fig-pos: "H"

from lucy_visualizations import plot_method_usage

fig = plot_method_usage(df)
```

```{python}
#| echo: false
#| code-fold: true
#| warning: false
#| output: asis

import os
from ai_analysis import generate_chart_commentary
from IPython.display import Markdown, display

method_usage_commentary = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        method_counts = df['method_pred'].value_counts().to_string()
        method_usage_commentary = generate_chart_commentary(
            "Grafico a barre orizzontali della distribuzione dell'uso dei metodi di riconoscimento",
            f"Distribuzione metodi:\n{method_counts}",
            domain="document"
        )
    else:
        method_usage_commentary = None
except (NameError, AttributeError):
    if os.getenv("OPENAI_API_KEY"):
        method_counts = df['method_pred'].value_counts().to_string()
        method_usage_commentary = generate_chart_commentary(
            "Grafico a barre orizzontali della distribuzione dell'uso dei metodi di riconoscimento",
            f"Distribuzione metodi:\n{method_counts}",
            domain="document"
        )
    else:
        method_usage_commentary = None
except Exception as e:
    method_usage_commentary = None

if method_usage_commentary:
    display(Markdown(f"**Commento AI:** {method_usage_commentary}"))
```

### Timeline Predizioni

```{python}
#| echo: false
#| label: fig-timeline
#| fig-cap: "Timeline delle predizioni nel tempo per metodo"
#| fig-width: 12
#| fig-height: 6
#| fig-pos: "H"

from lucy_visualizations import plot_timeline_predictions

fig = plot_timeline_predictions(df)
```

```{python}
#| echo: false
#| code-fold: true
#| warning: false
#| output: asis

import os
from ai_analysis import generate_chart_commentary
from IPython.display import Markdown, display

timeline_commentary = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        daily_summary = df.groupby([df['datetime_sent'].dt.date, 'method_pred']).size().reset_index(name='count')
        timeline_commentary = generate_chart_commentary(
            "Grafico timeline delle predizioni nel tempo per metodo",
            f"Periodo: {df['datetime_sent'].min()} - {df['datetime_sent'].max()}\nMetodi: {df['method_pred'].nunique()}",
            domain="document"
        )
    else:
        timeline_commentary = None
except (NameError, AttributeError):
    if os.getenv("OPENAI_API_KEY"):
        timeline_commentary = generate_chart_commentary(
            "Grafico timeline delle predizioni nel tempo per metodo",
            f"Periodo: {df['datetime_sent'].min()} - {df['datetime_sent'].max()}\nMetodi: {df['method_pred'].nunique()}",
            domain="document"
        )
    else:
        timeline_commentary = None
except Exception as e:
    timeline_commentary = None

if timeline_commentary:
    display(Markdown(f"**Commento AI:** {timeline_commentary}"))
```

## Analisi per Campo

```{python}
#| echo: false
#| code-fold: true

from data_loader import get_field_names, filter_by_field_name, calculate_metrics_by_method, calculate_percentage
from lucy_visualizations import plot_metrics_by_method, plot_confusion_matrix_by_method
from ai_analysis import analyze_data_summary, generate_chart_commentary
from table_formatter import format_table
from IPython.display import Markdown, display, HTML
import os

# Ottieni tutti i field_name
field_names = get_field_names(df) if 'field_name' in df.columns else []

# Per ogni field_name, crea una sottosezione
for field_name in field_names:
    field_df = filter_by_field_name(df, field_name)
    
    if len(field_df) == 0:
        continue
    
    display(Markdown(f"### Analisi: {field_name}"))
    
    # Statistiche per questo campo
    field_summary = {
        'Totale record': f"{len(field_df):,}",
        'Record validati': f"{field_df['is_validated'].sum():,} ({calculate_percentage(field_df['is_validated'].sum(), len(field_df), decimal_places=1):.1f}%)" if len(field_df) > 0 else "0",
        'Metodi utilizzati': f"{field_df['method_pred'].nunique()}" if 'method_pred' in field_df.columns else "0"
    }
    display(format_summary_dict(field_summary, f"Statistiche per {field_name}"))
    
    # Metriche per questo campo (se ci sono dati validati)
    if field_df['is_validated'].sum() > 0:
        field_metrics = calculate_metrics_by_method(field_df)
        if len(field_metrics) > 0:
            display(format_table(
                field_metrics[['method', 'method_type', 'precision', 'recall', 'f1', 'accuracy', 'total']],
                caption=f"Metriche di Performance per {field_name}",
                digits=3
            ))
            
            # Grafico metriche
            fig = plot_metrics_by_method(field_metrics)
            display(fig)
            
            # Commento AI per le metriche
            try:
                enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
                if enable_ai and os.getenv("OPENAI_API_KEY"):
                    metrics_summary = field_metrics[['method', 'precision', 'recall', 'f1', 'accuracy']].to_string()
                    metrics_commentary = generate_chart_commentary(
                        f"Grafico a barre delle metriche di performance per il campo {field_name}",
                        metrics_summary,
                        domain="document",
                        field_name=field_name
                    )
                    if metrics_commentary:
                        display(Markdown(f"**Commento AI:** {metrics_commentary}"))
            except Exception:
                pass
    
    # Analisi AI specifica per questo campo
    try:
        enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
        if enable_ai and os.getenv("OPENAI_API_KEY"):
            field_ai_summary = analyze_data_summary(field_df, field_name=field_name)
            if field_ai_summary:
                display(Markdown(f"**Analisi AI per {field_name}:**\n\n{field_ai_summary}"))
    except Exception:
        pass
    
    # Separatore orizzontale (usando HTML invece di markdown per evitare conflitti YAML)
    display(HTML("<hr>"))
```

## Performance Metodi

### Calcolo Metriche (Aggregate)

```{python}
#| echo: false
#| code-fold: true

from table_formatter import format_table
from IPython.display import display
from data_loader import calculate_metrics_by_field_and_method

# Calcola metriche per metodo (aggregate, tutti i field_name)
metrics_df = calculate_metrics_by_method(df)
display(format_table(
    metrics_df[['method', 'method_type', 'precision', 'recall', 'f1', 'accuracy', 'total']],
    caption="Metriche di Performance per Metodo (Aggregate - Tutti i Campi)",
    digits=3
))

# Calcola metriche per field_name e metodo (se field_name presente)
if 'field_name' in df.columns:
    field_method_metrics = calculate_metrics_by_field_and_method(df)
    if len(field_method_metrics) > 0:
        display(format_table(
            field_method_metrics[['field_name', 'method', 'precision', 'recall', 'f1', 'accuracy', 'total']],
            caption="Metriche di Performance per Campo e Metodo",
            digits=3
        ))
```

### Grafico Metriche

```{python}
#| echo: false
#| label: fig-metrics
#| fig-cap: "Metriche di performance (Precision, Recall, F1, Accuracy) per metodo"
#| fig-width: 12
#| fig-height: 6
#| fig-pos: "H"

from lucy_visualizations import plot_metrics_by_method

fig = plot_metrics_by_method(metrics_df)
```

```{python}
#| echo: false
#| code-fold: true
#| warning: false
#| output: asis

import os
from ai_analysis import generate_chart_commentary
from IPython.display import Markdown, display

metrics_commentary = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        metrics_summary = metrics_df[['method', 'precision', 'recall', 'f1', 'accuracy']].to_string()
        metrics_commentary = generate_chart_commentary(
            "Grafico a barre delle metriche di performance (precision, recall, F1, accuracy) per metodo di riconoscimento",
            metrics_summary,
            domain="document"
        )
    else:
        metrics_commentary = None
except (NameError, AttributeError):
    if os.getenv("OPENAI_API_KEY"):
        metrics_summary = metrics_df[['method', 'precision', 'recall', 'f1', 'accuracy']].to_string()
        metrics_commentary = generate_chart_commentary(
            "Grafico a barre delle metriche di performance (precision, recall, F1, accuracy) per metodo di riconoscimento",
            metrics_summary,
            domain="document"
        )
    else:
        metrics_commentary = None
except Exception as e:
    metrics_commentary = None

if metrics_commentary:
    display(Markdown(f"**Commento AI:** {metrics_commentary}"))
```

### Confronto ML vs Query-based

```{python}
#| echo: false
#| label: fig-ml-vs-query
#| fig-cap: "Confronto performance tra metodi ML, Query-based e Other"
#| fig-width: 10
#| fig-height: 6
#| fig-pos: "H"

from lucy_visualizations import plot_ml_vs_query_comparison

fig = plot_ml_vs_query_comparison(metrics_df)
```

```{python}
#| echo: false
#| code-fold: true
#| warning: false
#| output: asis

import os
from ai_analysis import generate_chart_commentary
from IPython.display import Markdown, display

ml_vs_query_commentary = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        type_metrics = metrics_df.groupby('method_type').agg({
            'precision': 'mean',
            'recall': 'mean',
            'f1': 'mean',
            'accuracy': 'mean'
        }).to_string()
        ml_vs_query_commentary = generate_chart_commentary(
            "Confronto performance tra metodi ML, Query-based e Other",
            type_metrics,
            domain="document"
        )
    else:
        ml_vs_query_commentary = None
except (NameError, AttributeError):
    if os.getenv("OPENAI_API_KEY"):
        type_metrics = metrics_df.groupby('method_type').agg({
            'precision': 'mean',
            'recall': 'mean',
            'f1': 'mean',
            'accuracy': 'mean'
        }).to_string()
        ml_vs_query_commentary = generate_chart_commentary(
            "Confronto performance tra metodi ML, Query-based e Other",
            type_metrics,
            domain="document"
        )
    else:
        ml_vs_query_commentary = None
except Exception as e:
    ml_vs_query_commentary = None

if ml_vs_query_commentary:
    display(Markdown(f"**Commento AI:** {ml_vs_query_commentary}"))
```

## Analisi Errori

### Distribuzione Errori

```{python}
#| echo: false
#| code-fold: true

from table_formatter import format_table, format_summary_dict
from IPython.display import display

validated = df[df['is_validated']].copy()

# Distribuzione errori (tutti i field_name)
error_dist = {
    'True Positive (TP)': f"{len(validated[validated['comparison'] == 'TP']):,}",
    'False Positive (FP)': f"{len(validated[validated['comparison'] == 'FP']):,}",
    'False Negative (FN)': f"{len(validated[validated['comparison'] == 'FN']):,}",
    'True Negative (TN)': f"{len(validated[validated['comparison'] == 'TN']):,}"
}
display(format_summary_dict(error_dist, "Distribuzione Errori (Tutti i Campi)"))

# Errori per metodo (tutti i field_name)
error_by_method = validated[validated['comparison'].isin(['FP', 'FN'])].groupby(['method_pred', 'comparison']).size().unstack(fill_value=0)
if len(error_by_method) > 0:
    display(format_table(error_by_method.reset_index(), caption="Errori per Metodo (Tutti i Campi)"))

# Errori per field_name (se presente)
if 'field_name' in validated.columns:
    error_by_field = validated[validated['comparison'].isin(['FP', 'FN'])].groupby(['field_name', 'comparison']).size().unstack(fill_value=0)
    if len(error_by_field) > 0:
        display(format_table(error_by_field.reset_index(), caption="Errori per Campo (field_name)"))
```

### Matrice di Confusione - Azure Model

```{python}
#| echo: false
#| label: fig-confusion-azure
#| fig-cap: "Matrice di confusione per Azure Model (ML/XGBoost)"
#| fig-width: 6
#| fig-height: 5
#| fig-pos: "H"

from lucy_visualizations import plot_confusion_matrix_by_method

fig = plot_confusion_matrix_by_method(df, 'azure_model')
```

```{python}
#| echo: false
#| code-fold: true
#| warning: false
#| output: asis

import os
from ai_analysis import generate_chart_commentary
from IPython.display import Markdown, display

confusion_commentary = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        method_data = df[(df['method_pred'] == 'azure_model') & (df['is_validated'])]
        if len(method_data) > 0:
            tp = len(method_data[method_data['comparison'] == 'TP'])
            fp = len(method_data[method_data['comparison'] == 'FP'])
            fn = len(method_data[method_data['comparison'] == 'FN'])
            tn = len(method_data[method_data['comparison'] == 'TN'])
            confusion_commentary = generate_chart_commentary(
                "Matrice di confusione per Azure Model (ML/XGBoost)",
                f"TP={tp}, FP={fp}, FN={fn}, TN={tn}",
                domain="document"
            )
    else:
        confusion_commentary = None
except (NameError, AttributeError):
    if os.getenv("OPENAI_API_KEY"):
        method_data = df[(df['method_pred'] == 'azure_model') & (df['is_validated'])]
        if len(method_data) > 0:
            tp = len(method_data[method_data['comparison'] == 'TP'])
            fp = len(method_data[method_data['comparison'] == 'FP'])
            fn = len(method_data[method_data['comparison'] == 'FN'])
            tn = len(method_data[method_data['comparison'] == 'TN'])
            confusion_commentary = generate_chart_commentary(
                "Matrice di confusione per Azure Model (ML/XGBoost)",
                f"TP={tp}, FP={fp}, FN={fn}, TN={tn}",
                domain="document"
            )
    else:
        confusion_commentary = None
except Exception as e:
    confusion_commentary = None

if confusion_commentary:
    display(Markdown(f"**Commento AI:** {confusion_commentary}"))
```

### Analisi Pattern di Errore

```{python}
#| echo: false
#| code-fold: true
#| warning: false

import os
from ai_analysis import analyze_error_patterns
from IPython.display import Markdown, display

error_analysis = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        error_analysis = analyze_error_patterns(df)
    elif not enable_ai:
        error_analysis = "Analisi AI disabilitata tramite parametro."
    else:
        error_analysis = "Analisi AI non disponibile."
except (NameError, AttributeError):
    if os.getenv("OPENAI_API_KEY"):
        error_analysis = analyze_error_patterns(df)
    else:
        error_analysis = "Analisi AI non disponibile."
except Exception as e:
    error_analysis = f"Errore: {str(e)}"
```

```{python}
#| echo: false
#| code-fold: true
#| output: asis

from IPython.display import Markdown

if error_analysis:
    display(Markdown(error_analysis))
```

## Distribuzione Confidence

```{python}
#| echo: false
#| label: fig-confidence
#| fig-cap: "Distribuzione della confidence per metodo"
#| fig-width: 12
#| fig-height: 6
#| fig-pos: "H"

from lucy_visualizations import plot_confidence_distribution

fig = plot_confidence_distribution(df)
```

```{python}
#| echo: false
#| code-fold: true
#| warning: false
#| output: asis

import os
from ai_analysis import generate_chart_commentary
from IPython.display import Markdown, display

confidence_commentary = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        df_with_conf = df[df['confidence'].notna()]
        if len(df_with_conf) > 0:
            conf_summary = f"Media confidence: {df_with_conf['confidence'].mean():.3f}\nStd: {df_with_conf['confidence'].std():.3f}"
            confidence_commentary = generate_chart_commentary(
                "Distribuzione della confidence per metodo (box plot)",
                conf_summary,
                domain="document"
            )
    else:
        confidence_commentary = None
except (NameError, AttributeError):
    if os.getenv("OPENAI_API_KEY"):
        df_with_conf = df[df['confidence'].notna()]
        if len(df_with_conf) > 0:
            conf_summary = f"Media confidence: {df_with_conf['confidence'].mean():.3f}\nStd: {df_with_conf['confidence'].std():.3f}"
            confidence_commentary = generate_chart_commentary(
                "Distribuzione della confidence per metodo (box plot)",
                conf_summary,
                domain="document"
            )
    else:
        confidence_commentary = None
except Exception as e:
    confidence_commentary = None

if confidence_commentary:
    display(Markdown(f"**Commento AI:** {confidence_commentary}"))
```

## Analisi Geografica

### Accuracy per Country e Metodo

```{python}
#| echo: false
#| label: fig-accuracy-heatmap
#| fig-cap: "Heatmap dell'accuratezza per country e metodo"
#| fig-width: 14
#| fig-height: 10
#| fig-pos: "H"

from lucy_visualizations import plot_accuracy_heatmap

fig = plot_accuracy_heatmap(df)
```

```{python}
#| echo: false
#| code-fold: true
#| warning: false
#| output: asis

import os
from ai_analysis import generate_chart_commentary
from IPython.display import Markdown, display

heatmap_commentary = None
try:
    enable_ai = params.enable_ai if hasattr(params, 'enable_ai') else True
    if enable_ai and os.getenv("OPENAI_API_KEY"):
        validated = df[df['is_validated']].copy()
        if len(validated) > 0:
            countries = validated['country'].dropna().unique()
            methods = validated['method_pred'].dropna().unique()
            heatmap_commentary = generate_chart_commentary(
                "Heatmap dell'accuratezza per country e metodo",
                f"Countries: {len(countries)}, Metodi: {len(methods)}",
                domain="document"
            )
    else:
        heatmap_commentary = None
except (NameError, AttributeError):
    if os.getenv("OPENAI_API_KEY"):
        validated = df[df['is_validated']].copy()
        if len(validated) > 0:
            countries = validated['country'].dropna().unique()
            methods = validated['method_pred'].dropna().unique()
            heatmap_commentary = generate_chart_commentary(
                "Heatmap dell'accuratezza per country e metodo",
                f"Countries: {len(countries)}, Metodi: {len(methods)}",
                domain="document"
            )
    else:
        heatmap_commentary = None
except Exception as e:
    heatmap_commentary = None

if heatmap_commentary:
    display(Markdown(f"**Commento AI:** {heatmap_commentary}"))
```

## Modelli LLM Utilizzati

```{python}
#| echo: false
#| output: asis

import sys
sys.path.append('../src')

from model_tracker import ModelTracker
from ai_analysis import get_model_display_name, get_llm_with_fallback
from IPython.display import Markdown, display

tracker = ModelTracker()
llm_config = get_llm_with_fallback()

# Ottieni gerarchia configurata
if llm_config:
    fallback_models = llm_config.get("fallback_models", [])
    hierarchy = " → ".join([get_model_display_name(m) for m in fallback_models])
else:
    hierarchy = "N/A"

# Statistiche utilizzo
stats = tracker.get_usage_stats()
primary = tracker.get_primary_model()
total_calls = tracker.get_total_calls()

# Mostra informazioni
if total_calls > 0:
    stats_text = "\n".join([f"- **{get_model_display_name(m)}**: {c} chiamate" for m, c in sorted(stats.items(), key=lambda x: x[1], reverse=True)])
    primary_display = get_model_display_name(primary) if primary else "Nessuna chiamata registrata"
    
    model_info = f"""
**Gerarchia configurata:** {hierarchy}

**Modello primario utilizzato:** {primary_display}

**Statistiche utilizzo:**
{stats_text}

**Totale chiamate LLM:** {total_calls}
"""
else:
    model_info = f"""
**Gerarchia configurata:** {hierarchy}

**Modello primario utilizzato:** Nessuna chiamata registrata

*Nota: Le statistiche di utilizzo saranno disponibili dopo l'esecuzione delle analisi AI.*
"""

display(Markdown(model_info))
```

## Conclusioni e Raccomandazioni

### Metriche Chiave

```{python}
#| echo: false
#| code-fold: true

from table_formatter import format_table, format_summary_dict
from IPython.display import display

# Riepilogo Performance
summary_perf = {
    'Accuracy media': f"{metrics_df['accuracy'].mean():.3f}",
    'Precision media': f"{metrics_df['precision'].mean():.3f}",
    'Recall media': f"{metrics_df['recall'].mean():.3f}",
    'F1 media': f"{metrics_df['f1'].mean():.3f}",
    'Metodo migliore (F1)': f"{metrics_df.loc[metrics_df['f1'].idxmax(), 'method']} (F1={metrics_df['f1'].max():.3f})",
    'Metodo più usato': f"{df['method_pred'].value_counts().index[0]} ({df['method_pred'].value_counts().iloc[0]:,} predizioni)"
}
display(format_summary_dict(summary_perf, "Riepilogo Performance"))

# Confronto ML vs Query
ml_metrics = metrics_df[metrics_df['method_type'] == 'ML']
query_metrics = metrics_df[metrics_df['method_type'] == 'Query']

if len(ml_metrics) > 0 and len(query_metrics) > 0:
    comparison_data = {
        'Tipo': ['ML', 'Query'],
        'Accuracy media': [f"{ml_metrics['accuracy'].mean():.3f}", f"{query_metrics['accuracy'].mean():.3f}"],
        'F1 media': [f"{ml_metrics['f1'].mean():.3f}", f"{query_metrics['f1'].mean():.3f}"]
    }
    comparison_df = pd.DataFrame(comparison_data)
    display(format_table(comparison_df, caption="Confronto ML vs Query"))
```

### Raccomandazioni

```{python}
#| echo: false
#| code-fold: true
#| warning: false

from ai_analysis import generate_section_text

recommendations = None
try:
    if os.getenv("OPENAI_API_KEY"):
        context = f"""
Metriche calcolate:
{metrics_df[['method', 'method_type', 'precision', 'recall', 'f1', 'accuracy']].to_string()}

Errori totali: FP={len(validated[validated['comparison'] == 'FP'])}, FN={len(validated[validated['comparison'] == 'FN'])}
"""
        recommendations = generate_section_text(
            "Raccomandazioni per il miglioramento del sistema di riconoscimento documentale",
            context
        )
    else:
        recommendations = "Analisi AI non disponibile per le raccomandazioni."
except Exception as e:
    recommendations = f"Errore: {str(e)}"
```

```{python}
#| echo: false
#| code-fold: true
#| output: asis

from IPython.display import Markdown, display

if recommendations:
    display(Markdown(recommendations))
```
